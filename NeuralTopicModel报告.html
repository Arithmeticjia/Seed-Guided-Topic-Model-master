<!DOCTYPE html>
<html><head>
		<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
		<title>Zotero 报告</title>
		<link rel="stylesheet" type="text/css" href="data:text/css;base64,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">
		<link rel="stylesheet" type="text/css" media="screen,projection" href="data:text/css;base64,LyogR2VuZXJpYyBzdHlsZXMgKi8KYm9keSB7Cglmb250OiA2Mi41JSBHZW9yZ2lhLCBUaW1lcywgc2VyaWY7Cgl3aWR0aDogNzgwcHg7CgltYXJnaW46IDAgYXV0bzsKfQoKaDIgewoJZm9udC1zaXplOiAxLjVlbTsKCWxpbmUtaGVpZ2h0OiAxLjVlbTsKCWZvbnQtZmFtaWx5OiBHZW9yZ2lhLCBUaW1lcywgc2VyaWY7Cn0KCnAgewoJbGluZS1oZWlnaHQ6IDEuNWVtOwp9CgphOmxpbmssIGE6dmlzaXRlZCB7Cgljb2xvcjogIzkwMDsKfQoKYTpob3ZlciwgYTphY3RpdmUgewoJY29sb3I6ICM3Nzc7Cn0KCgp1bC5yZXBvcnQgewoJZm9udC1zaXplOiAxLjRlbTsKCXdpZHRoOiA2ODBweDsKCW1hcmdpbjogMCBhdXRvOwoJcGFkZGluZzogMjBweCAyMHB4Owp9CgovKiBNZXRhZGF0YSB0YWJsZSAqLwp0YWJsZSB7Cglib3JkZXI6IDFweCAjY2NjIHNvbGlkOwoJb3ZlcmZsb3c6IGF1dG87Cgl3aWR0aDogMTAwJTsKCW1hcmdpbjogLjFlbSBhdXRvIC43NWVtOwoJcGFkZGluZzogMC41ZW07Cn0K">
		<link rel="stylesheet" type="text/css" media="print" href="data:text/css;base64,Ym9keSB7Cglmb250OiAxMnB0ICJUaW1lcyBOZXcgUm9tYW4iLCBUaW1lcywgR2VvcmdpYSwgc2VyaWY7CgltYXJnaW46IDA7Cgl3aWR0aDogYXV0bzsKCWNvbG9yOiBibGFjazsKfQoKLyogUGFnZSBCcmVha3MgKHBhZ2UtYnJlYWstaW5zaWRlIG9ubHkgcmVjb2duaXplZCBieSBPcGVyYSkgKi8KaDEsIGgyLCBoMywgaDQsIGg1LCBoNiB7CglwYWdlLWJyZWFrLWFmdGVyOiBhdm9pZDsKCXBhZ2UtYnJlYWstaW5zaWRlOiBhdm9pZDsKfQoKdWwsIG9sLCBkbCB7CglwYWdlLWJyZWFrLWluc2lkZTogYXZvaWQ7Cgljb2xvci1hZGp1c3Q6IGV4YWN0Owp9CgpoMiB7Cglmb250LXNpemU6IDEuM2VtOwoJbGluZS1oZWlnaHQ6IDEuM2VtOwp9CgphIHsKCWNvbG9yOiAjMDAwOwoJdGV4dC1kZWNvcmF0aW9uOiBub25lOwp9Cg==">
	</head>
	<body>
		<ul class="report combineChildItems">
			<li id="item_3MX5NXUV" class="item journalArticle">
			<h2>A Neural Framework for Generalized Topic Models</h2>
				<table>
					<tbody><tr>
						<th>类型</th>
						<td>期刊文章</td>
					</tr>
					<tr>
						<th class="author">作者</th>
						<td>Dallas Card</td>
					</tr>
					<tr>
						<th class="author">作者</th>
						<td>Chenhao Tan</td>
					</tr>
					<tr>
						<th class="author">作者</th>
						<td>Noah A. Smith</td>
					</tr>
					<tr>
					<th>卷</th>
						<td>abs/1705.09296</td>
					</tr>
					<tr>
					<th>期刊</th>
						<td>CoRR</td>
					</tr>
					<tr>
					<th>日期</th>
						<td>2017</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.18653/v1/P18-1189">10.18653/v1/P18-1189</a></td>
					</tr>
					<tr>
					<th>馆藏目录</th>
						<td>Semantic Scholar</td>
					</tr>
					<tr>
					<th>摘要</th>
						<td>Topic models for text corpora comprise a popular family of 
methods that have inspired many extensions to encode properties such as 
sparsity, interactions with covariates, and the gradual evolution of 
topics. In this paper, we combine certain motivating ideas behind 
variations on topic models with modern techniques for variational 
inference to produce a flexible framework for topic modeling that allows
 for rapid exploration of different models. We first discuss how our 
framework relates to existing models, and then demonstrate that it 
achieves strong performance, with the introduction of sparsity 
controlling the trade off between perplexity and topic coherence. We 
have released our code and preprocessing scripts to support easy future 
comparisons and exploration.</td>
					</tr>
					<tr>
					<th>添加日期</th>
						<td>2019/9/2 下午4:24:20</td>
					</tr>
					<tr>
					<th>修改日期</th>
						<td>2019/9/2 下午4:34:56</td>
					</tr>
				</tbody></table>
				<h3 class="tags">标签：</h3>
				<ul class="tags">
					<li>Statistics - Machine Learning</li>
					<li>Computer Science - Computation and Language</li>
					<li>Topic model</li>
					<li>Interaction</li>
					<li>Sparse matrix</li>
					<li>ENCODE</li>
					<li>Off topic</li>
					<li>Perplexity</li>
					<li>Preprocessor</li>
					<li>Text corpus</li>
					<li>Variational principle</li>
				</ul>
				<h3 class="notes">笔记：</h3>
				<ul class="notes">
					<li id="item_N74JTBSB">
<div><p>Comment: 13 pages, 3 figures, 6 tables; updating to version published at ACL 2018</p></div>
					</li>
				</ul>
				<h3 class="attachments">附件</h3>
				<ul class="attachments">
					<li id="item_WZSV8A9B">arXiv.org Snapshot					</li>
					<li id="item_JFZ9ZB9P">Card et al_2017_A Neural Framework for Generalized Topic Models.pdf					</li>
					<li id="item_M8ASMYED">Card et al_2017_Neural Models for Documents with Metadata.pdf					</li>
					<li id="item_REZKKA94">Semantic Scholar Link					</li>
				</ul>
			</li>


			<li id="item_NRTRIJLH" class="item journalArticle">
			<h2>A Novel Neural Topic Model and Its Supervised Extension</h2>
				<table>
					<tbody><tr>
						<th>类型</th>
						<td>期刊文章</td>
					</tr>
					<tr>
						<th class="author">作者</th>
						<td>Ziqiang Cao</td>
					</tr>
					<tr>
						<th class="author">作者</th>
						<td>Sujian Li</td>
					</tr>
					<tr>
						<th class="author">作者</th>
						<td>Yang Liu</td>
					</tr>
					<tr>
						<th class="author">作者</th>
						<td>Wenjie Li</td>
					</tr>
					<tr>
						<th class="author">作者</th>
						<td>Heng Ji</td>
					</tr>
					<tr>
					<th>页码</th>
						<td>7</td>
					</tr>
					<tr>
					<th>馆藏目录</th>
						<td>Zotero</td>
					</tr>
					<tr>
					<th>语言</th>
						<td>en</td>
					</tr>
					<tr>
					<th>摘要</th>
						<td>Topic modeling techniques have the beneﬁts of modeling words 
and documents uniformly under a probabilistic framework. However, they 
also suffer from the limitations of sensitivity to initialization and 
unigram topic distribution, which can be remedied by deep learning 
techniques. To explore the combination of topic modeling and deep 
learning techniques, we ﬁrst explain the standard topic model from the 
perspective of a neural network. Based on this, we propose a novel 
neural topic model (NTM) where the representation of words and documents
 are efﬁciently and naturally combined into a uniform framework. 
Extending from NTM, we can easily add a label layer and propose the 
supervised neural topic model (sNTM) to tackle supervised tasks. 
Experiments show that our models are competitive in both topic discovery
 and classiﬁcation/regression tasks.</td>
					</tr>
					<tr>
					<th>添加日期</th>
						<td>2019/6/3 下午4:29:44</td>
					</tr>
					<tr>
					<th>修改日期</th>
						<td>2019/6/3 下午4:29:44</td>
					</tr>
				</tbody></table>
				<h3 class="attachments">附件</h3>
				<ul class="attachments">
					<li id="item_WCDG3SH3">Cao 等。 - A Novel Neural Topic Model and Its Supervised Exte.pdf					</li>
				</ul>
			</li>


			<li id="item_9ZMJZZFA" class="item journalArticle">
			<h2>Coherence-Aware Neural Topic Modeling</h2>
				<table>
					<tbody><tr>
						<th>类型</th>
						<td>期刊文章</td>
					</tr>
					<tr>
						<th class="author">作者</th>
						<td>Ran Ding</td>
					</tr>
					<tr>
						<th class="author">作者</th>
						<td>Ramesh Nallapati</td>
					</tr>
					<tr>
						<th class="author">作者</th>
						<td>Bing Xiang</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://arxiv.org/abs/1809.02687">http://arxiv.org/abs/1809.02687</a></td>
					</tr>
					<tr>
					<th>期刊</th>
						<td>arXiv:1809.02687 [cs]</td>
					</tr>
					<tr>
					<th>日期</th>
						<td>2018-09-07</td>
					</tr>
					<tr>
					<th>其它</th>
						<td>arXiv: 1809.02687</td>
					</tr>
					<tr>
					<th>访问时间</th>
						<td>2019/9/2 下午4:14:40</td>
					</tr>
					<tr>
					<th>馆藏目录</th>
						<td>arXiv.org</td>
					</tr>
					<tr>
					<th>摘要</th>
						<td>Topic models are evaluated based on their ability to describe 
documents well (i.e. low perplexity) and to produce topics that carry 
coherent semantic meaning. In topic modeling so far, perplexity is a 
direct optimization target. However, topic coherence, owing to its 
challenging computation, is not optimized for and is only evaluated 
after training. In this work, under a neural variational inference 
framework, we propose methods to incorporate a topic coherence objective
 into the training process. We demonstrate that such a coherence-aware 
topic model exhibits a similar level of perplexity as baseline models 
but achieves substantially higher topic coherence.</td>
					</tr>
					<tr>
					<th>添加日期</th>
						<td>2019/9/2 下午4:14:40</td>
					</tr>
					<tr>
					<th>修改日期</th>
						<td>2019/9/15 上午12:50:55</td>
					</tr>
				</tbody></table>
				<h3 class="tags">标签：</h3>
				<ul class="tags">
					<li>Computer Science - Computation and Language</li>
					<li>Computer Science - Machine Learning</li>
				</ul>
				<h3 class="notes">笔记：</h3>
				<ul class="notes">
					<li id="item_Q58K8GZ9">
<p class="plaintext">Comment: Accepted at EMNLP 2018</p>
					</li>
				</ul>
				<h3 class="attachments">附件</h3>
				<ul class="attachments">
					<li id="item_KZ9BMVMS">arXiv.org Snapshot					</li>
					<li id="item_3RLRTZ7H">Ding et al_2018_Coherence-Aware Neural Topic Modeling.pdf					</li>
				</ul>
			</li>


			<li id="item_WFVAMZU7" class="item journalArticle">
			<h2>Document Informed Neural Autoregressive Topic Models with Distributional Prior</h2>
				<table>
					<tbody><tr>
						<th>类型</th>
						<td>期刊文章</td>
					</tr>
					<tr>
						<th class="author">作者</th>
						<td>Pankaj Gupta</td>
					</tr>
					<tr>
						<th class="author">作者</th>
						<td>Yatin Chaudhary</td>
					</tr>
					<tr>
						<th class="author">作者</th>
						<td>Florian Buettner</td>
					</tr>
					<tr>
						<th class="author">作者</th>
						<td>Hinrich Schütze</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://arxiv.org/abs/1809.06709">http://arxiv.org/abs/1809.06709</a></td>
					</tr>
					<tr>
					<th>期刊</th>
						<td>arXiv:1809.06709 [cs]</td>
					</tr>
					<tr>
					<th>日期</th>
						<td>2018-09-15</td>
					</tr>
					<tr>
					<th>其它</th>
						<td>arXiv: 1809.06709</td>
					</tr>
					<tr>
					<th>访问时间</th>
						<td>2019/9/16 上午12:44:25</td>
					</tr>
					<tr>
					<th>馆藏目录</th>
						<td>arXiv.org</td>
					</tr>
					<tr>
					<th>摘要</th>
						<td>We address two challenges in topic models: (1) Context 
information around words helps in determining their actual meaning, 
e.g., "networks" used in the contexts "artificial neural networks" vs. 
"biological neuron networks". Generative topic models infer topic-word 
distributions, taking no or only little context into account. Here, we 
extend a neural autoregressive topic model to exploit the full context 
information around words in a document in a language modeling fashion. 
The proposed model is named as iDocNADE. (2) Due to the small number of 
word occurrences (i.e., lack of context) in short text and data sparsity
 in a corpus of few documents, the application of topic models is 
challenging on such texts. Therefore, we propose a simple and efficient 
way of incorporating external knowledge into neural autoregressive topic
 models: we use embeddings as a distributional prior. The proposed 
variants are named as DocNADEe and iDocNADEe. We present novel neural 
autoregressive topic model variants that consistently outperform 
state-of-the-art generative topic models in terms of generalization, 
interpretability (topic coherence) and applicability (retrieval and 
classification) over 7 long-text and 8 short-text datasets from diverse 
domains.</td>
					</tr>
					<tr>
					<th>添加日期</th>
						<td>2019/9/16 上午12:44:25</td>
					</tr>
					<tr>
					<th>修改日期</th>
						<td>2019/9/16 上午12:44:25</td>
					</tr>
				</tbody></table>
				<h3 class="tags">标签：</h3>
				<ul class="tags">
					<li>Computer Science - Artificial Intelligence</li>
					<li>Computer Science - Information Retrieval</li>
					<li>Computer Science - Computation and Language</li>
					<li>Computer Science - Machine Learning</li>
				</ul>
				<h3 class="notes">笔记：</h3>
				<ul class="notes">
					<li id="item_7EQDPFUG">
<p class="plaintext">Comment: AAAI2019. arXiv admin note: substantial text overlap with arXiv:1808.03793</p>
					</li>
				</ul>
				<h3 class="attachments">附件</h3>
				<ul class="attachments">
					<li id="item_D7GIBNM8">arXiv:1809.06709 PDF					</li>
					<li id="item_42LZPF3B">arXiv.org Snapshot					</li>
				</ul>
			</li>


			<li id="item_ZC3LWRSK" class="item journalArticle">
			<h2>Learning document representation via topic-enhanced LSTM model</h2>
				<table>
					<tbody><tr>
						<th>类型</th>
						<td>期刊文章</td>
					</tr>
					<tr>
						<th class="author">作者</th>
						<td>Wenyue Zhang</td>
					</tr>
					<tr>
						<th class="author">作者</th>
						<td>Yang Li</td>
					</tr>
					<tr>
						<th class="author">作者</th>
						<td>Suge Wang</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://www.sciencedirect.com/science/article/pii/S0950705119301182">http://www.sciencedirect.com/science/article/pii/S0950705119301182</a></td>
					</tr>
					<tr>
					<th>卷</th>
						<td>174</td>
					</tr>
					<tr>
					<th>页码</th>
						<td>194-204</td>
					</tr>
					<tr>
					<th>期刊</th>
						<td>Knowledge-Based Systems</td>
					</tr>
					<tr>
					<th>ISSN</th>
						<td>0950-7051</td>
					</tr>
					<tr>
					<th>日期</th>
						<td>June 15, 2019</td>
					</tr>
					<tr>
					<th>刊名缩写</th>
						<td>Knowledge-Based Systems</td>
					</tr>
					<tr>
					<th>DOI</th>
						<td><a href="http://doi.org/10.1016/j.knosys.2019.03.007">10.1016/j.knosys.2019.03.007</a></td>
					</tr>
					<tr>
					<th>访问时间</th>
						<td>2019/9/16 上午12:44:37</td>
					</tr>
					<tr>
					<th>馆藏目录</th>
						<td>ScienceDirect</td>
					</tr>
					<tr>
					<th>摘要</th>
						<td>Document representation plays an important role in the fields 
of text mining, natural language processing, and information retrieval. 
Traditional approaches to document representation may suffer from the 
disregard of the correlations or order of words in a document, due to 
unrealistic assumption of word independence or exchangeability. 
Recently, long–short-term memory (LSTM) based recurrent neural networks 
have been shown effective in preserving local contextual sequential 
patterns of words in a document, but using the LSTM model alone may not 
be adequate to capture global topical semantics for learning document 
representation. In this work, we propose a new topic-enhanced LSTM model
 to deal with the document representation problem. We first employ an 
attention-based LSTM model to generate hidden representation of word 
sequence in a given document. Then, we introduce a latent topic modeling
 layer with similarity constraint on the local hidden representation, 
and build a tree-structured LSTM on top of the topic layer for 
generating semantic representation of the document. We evaluate our 
model in typical text mining applications, i.e., document 
classification, topic detection, information retrieval, and document 
clustering. Experimental results on real-world datasets show the benefit
 of our innovations over state-of-the-art baseline methods.</td>
					</tr>
					<tr>
					<th>添加日期</th>
						<td>2019/9/16 上午12:44:37</td>
					</tr>
					<tr>
					<th>修改日期</th>
						<td>2019/10/15 下午7:41:08</td>
					</tr>
				</tbody></table>
				<h3 class="tags">标签：</h3>
				<ul class="tags">
					<li>Deep learning</li>
					<li>Topic modeling</li>
					<li>Document representation</li>
					<li>Long–short term memory</li>
				</ul>
				<h3 class="attachments">附件</h3>
				<ul class="attachments">
					<li id="item_WTJKJIPA">ScienceDirect Snapshot					</li>
					<li id="item_EFFK6LIC">ScienceDirect Snapshot					</li>
					<li id="item_IFM3F3UR">Zhang et al_2019_Learning document representation via topic-enhanced LSTM model.pdf					</li>
				</ul>
			</li>


			<li id="item_Y724VV7P" class="item journalArticle">
			<h2>Learning from LDA using Deep Neural Networks</h2>
				<table>
					<tbody><tr>
						<th>类型</th>
						<td>期刊文章</td>
					</tr>
					<tr>
						<th class="author">作者</th>
						<td>Dongxu Zhang</td>
					</tr>
					<tr>
						<th class="author">作者</th>
						<td>Tianyi Luo</td>
					</tr>
					<tr>
						<th class="author">作者</th>
						<td>Dong Wang</td>
					</tr>
					<tr>
						<th class="author">作者</th>
						<td>Rong Liu</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://arxiv.org/abs/1508.01011v1">https://arxiv.org/abs/1508.01011v1</a></td>
					</tr>
					<tr>
					<th>日期</th>
						<td>2015/08/05</td>
					</tr>
					<tr>
					<th>访问时间</th>
						<td>2019/8/30 上午11:18:55</td>
					</tr>
					<tr>
					<th>馆藏目录</th>
						<td>arxiv.org</td>
					</tr>
					<tr>
					<th>语言</th>
						<td>en</td>
					</tr>
					<tr>
					<th>摘要</th>
						<td>Latent Dirichlet Allocation (LDA) is a three-level hierarchical Bayesian
model for topic inference. In spite of its great success, inferring the latent
topic distribution with LDA is time-consuming. Motivated by the transfer
learning approach proposed by~\newcite{hinton2015distilling}, we present a
novel method that uses LDA to supervise the training of a deep neural network
(DNN), so that the DNN can approximate the costly LDA inference with less
computation. Our experiments on a document classification task show that a
simple DNN can learn the LDA behavior pretty well, while the inference is
speeded up tens or hundreds of times.</td>
					</tr>
					<tr>
					<th>添加日期</th>
						<td>2019/8/30 上午11:18:55</td>
					</tr>
					<tr>
					<th>修改日期</th>
						<td>2019/8/30 上午11:18:55</td>
					</tr>
				</tbody></table>
				<h3 class="attachments">附件</h3>
				<ul class="attachments">
					<li id="item_GGB7TC53">Snapshot					</li>
					<li id="item_WKMHIK6P">Zhang et al_2015_Learning from LDA using Deep Neural Networks.pdf					</li>
				</ul>
			</li>


			<li id="item_RDI2E2VD" class="item journalArticle">
			<h2>Topic Compositional Neural Language Model</h2>
				<table>
					<tbody><tr>
						<th>类型</th>
						<td>期刊文章</td>
					</tr>
					<tr>
						<th class="author">作者</th>
						<td>Wenlin Wang</td>
					</tr>
					<tr>
						<th class="author">作者</th>
						<td>Zhe Gan</td>
					</tr>
					<tr>
						<th class="author">作者</th>
						<td>Wenqi Wang</td>
					</tr>
					<tr>
						<th class="author">作者</th>
						<td>Dinghan Shen</td>
					</tr>
					<tr>
						<th class="author">作者</th>
						<td>Jiaji Huang</td>
					</tr>
					<tr>
						<th class="author">作者</th>
						<td>Wei Ping</td>
					</tr>
					<tr>
						<th class="author">作者</th>
						<td>Sanjeev Satheesh</td>
					</tr>
					<tr>
						<th class="author">作者</th>
						<td>Lawrence Carin</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="http://arxiv.org/abs/1712.09783">http://arxiv.org/abs/1712.09783</a></td>
					</tr>
					<tr>
					<th>期刊</th>
						<td>arXiv:1712.09783 [cs]</td>
					</tr>
					<tr>
					<th>日期</th>
						<td>2017-12-28</td>
					</tr>
					<tr>
					<th>其它</th>
						<td>arXiv: 1712.09783</td>
					</tr>
					<tr>
					<th>访问时间</th>
						<td>2019/9/3 下午5:11:37</td>
					</tr>
					<tr>
					<th>馆藏目录</th>
						<td>arXiv.org</td>
					</tr>
					<tr>
					<th>摘要</th>
						<td>We propose a Topic Compositional Neural Language Model 
(TCNLM), a novel method designed to simultaneously capture both the 
global semantic meaning and the local word ordering structure in a 
document. The TCNLM learns the global semantic coherence of a document 
via a neural topic model, and the probability of each learned latent 
topic is further used to build a Mixture-of-Experts (MoE) language 
model, where each expert (corresponding to one topic) is a recurrent 
neural network (RNN) that accounts for learning the local structure of a
 word sequence. In order to train the MoE model efficiently, a matrix 
factorization method is applied, by extending each weight matrix of the 
RNN to be an ensemble of topic-dependent weight matrices. The degree to 
which each member of the ensemble is used is tied to the 
document-dependent probability of the corresponding topics. Experimental
 results on several corpora show that the proposed approach outperforms 
both a pure RNN-based model and other topic-guided language models. 
Further, our model yields sensible topics, and also has the capacity to 
generate meaningful sentences conditioned on given topics.</td>
					</tr>
					<tr>
					<th>添加日期</th>
						<td>2019/9/3 下午5:11:37</td>
					</tr>
					<tr>
					<th>修改日期</th>
						<td>2019/9/3 下午5:11:37</td>
					</tr>
				</tbody></table>
				<h3 class="tags">标签：</h3>
				<ul class="tags">
					<li>Computer Science - Computation and Language</li>
					<li>Computer Science - Machine Learning</li>
				</ul>
				<h3 class="notes">笔记：</h3>
				<ul class="notes">
					<li id="item_7Q2JU2MB">
<div><p>Comment: To appear in AISTATS 2018, updated version</p></div>
					</li>
				</ul>
				<h3 class="attachments">附件</h3>
				<ul class="attachments">
					<li id="item_86Q7XCMV">arXiv.org Snapshot					</li>
					<li id="item_GFKUC6TT">Wang et al_2017_Topic Compositional Neural Language Model.pdf					</li>
				</ul>
			</li>


			<li id="item_S5QHJMNE" class="item journalArticle">
			<h2>TOPICRNN: A RECURRENT NEURAL NETWORK WITH LONG-RANGE SEMANTIC DEPENDENCY</h2>
				<table>
					<tbody><tr>
						<th>类型</th>
						<td>期刊文章</td>
					</tr>
					<tr>
						<th class="author">作者</th>
						<td>Adji B Dieng</td>
					</tr>
					<tr>
						<th class="author">作者</th>
						<td>Chong Wang</td>
					</tr>
					<tr>
						<th class="author">作者</th>
						<td>Jianfeng Gao</td>
					</tr>
					<tr>
						<th class="author">作者</th>
						<td>John Paisley</td>
					</tr>
					<tr>
					<th>页码</th>
						<td>13</td>
					</tr>
					<tr>
					<th>日期</th>
						<td>2017</td>
					</tr>
					<tr>
					<th>馆藏目录</th>
						<td>Zotero</td>
					</tr>
					<tr>
					<th>语言</th>
						<td>en</td>
					</tr>
					<tr>
					<th>摘要</th>
						<td>In this paper, we propose TopicRNN, a recurrent neural network
 (RNN)-based language model designed to directly capture the global 
semantic meaning relating words in a document via latent topics. Because
 of their sequential nature, RNNs are good at capturing the local 
structure of a word sequence – both semantic and syntactic – but might 
face difﬁculty remembering long-range dependencies. Intuitively, these 
long-range dependencies are of semantic nature. In contrast, latent 
topic models are able to capture the global semantic structure of a 
document but do not account for word ordering. The proposed TopicRNN 
model integrates the merits of RNNs and latent topic models: it captures
 local (syntactic) dependencies using an RNN and global (semantic) 
dependencies using latent topics. Unlike previous work on contextual RNN
 language modeling, our model is learned endto-end. Empirical results on
 word prediction show that TopicRNN outperforms existing contextual RNN 
baselines. In addition, TopicRNN can be used as an unsupervised feature 
extractor for documents. We do this for sentiment analysis on the IMDB 
movie review dataset and report an error rate of 6.28%. This is 
comparable to the state-of-the-art 5.91% resulting from a 
semi-supervised approach. Finally, TopicRNN also yields sensible topics,
 making it a useful alternative to document models such as latent 
Dirichlet allocation.</td>
					</tr>
					<tr>
					<th>添加日期</th>
						<td>2019/8/30 上午11:21:52</td>
					</tr>
					<tr>
					<th>修改日期</th>
						<td>2019/9/2 下午4:34:49</td>
					</tr>
				</tbody></table>
				<h3 class="tags">标签：</h3>
				<ul class="tags">
					<li>Computer Science - Artificial Intelligence</li>
					<li>Statistics - Machine Learning</li>
					<li>Computer Science - Computation and Language</li>
					<li>Computer Science - Machine Learning</li>
				</ul>
				<h3 class="notes">笔记：</h3>
				<ul class="notes">
					<li id="item_MURLNK4W">
<p class="plaintext">Comment: International Conference on Learning Representations</p>
					</li>
				</ul>
				<h3 class="attachments">附件</h3>
				<ul class="attachments">
					<li id="item_U8ZVXZVU">arXiv.org Snapshot					</li>
					<li id="item_HQ46KF3B">Dieng 等。 - 2017 - TOPICRNN A RECURRENT NEURAL NETWORK WITH LONG-RAN.pdf					</li>
				</ul>
			</li>


			<li id="item_L9K6M5ML" class="item conferencePaper">
			<h2>Variational Autoencoder for Semi-Supervised Text Classification</h2>
				<table>
					<tbody><tr>
						<th>类型</th>
						<td>会议论文</td>
					</tr>
					<tr>
						<th class="author">作者</th>
						<td>Weidi Xu</td>
					</tr>
					<tr>
						<th class="author">作者</th>
						<td>Haoze Sun</td>
					</tr>
					<tr>
						<th class="author">作者</th>
						<td>Chao Deng</td>
					</tr>
					<tr>
						<th class="author">作者</th>
						<td>Ying Tan</td>
					</tr>
					<tr>
					<th>URL</th>
						<td><a href="https://www.aaai.org/ocs/index.php/AAAI/AAAI17/paper/view/14299">https://www.aaai.org/ocs/index.php/AAAI/AAAI17/paper/view/14299</a></td>
					</tr>
					<tr>
					<th>版权</th>
						<td>Authors who publish a paper in this conference agree to the 
following terms:   Author(s) agree to transfer their copyrights in their
 article/paper to the Association for the Advancement of Artificial 
Intelligence (AAAI), in order to deal with future requests for reprints,
 translations, anthologies, reproductions, excerpts, and other 
publications. This grant will include, without limitation, the entire 
copyright in the article/paper in all countries of the world, including 
all renewals, extensions, and reversions thereof, whether such rights 
current exist or hereafter come into effect, and also the exclusive 
right to create electronic versions of the article/paper, to the extent 
that such right is not subsumed under copyright.  The author(s) warrants
 that they are the sole author and owner of the copyright in the above 
article/paper, except for those portions shown to be in quotations; that
 the article/paper is original throughout; and that the undersigned 
right to make the grants set forth above is complete and unencumbered.  
The author(s) agree that if anyone brings any claim or action alleging 
facts that, if true, constitute a breach of any of the foregoing 
warranties, the author(s) will hold harmless and indemnify AAAI, their 
grantees, their licensees, and their distributors against any liability,
 whether under judgment, decree, or compromise, and any legal fees and 
expenses arising out of that claim or actions, and the undersigned will 
cooperate fully in any defense AAAI may make to such claim or action. 
Moreover, the undersigned agrees to cooperate in any claim or other 
action seeking to protect or enforce any right the undersigned has 
granted to AAAI in the article/paper. If any such claim or action fails 
because of facts that constitute a breach of any of the foregoing 
warranties, the undersigned agrees to reimburse whomever brings such 
claim or action for expenses and attorneys’ fees incurred therein.  
Author(s) retain all proprietary rights other than copyright (such as 
patent rights).  Author(s) may make personal reuse of all or portions of
 the above article/paper in other works of their own authorship.  
Author(s) may reproduce, or have reproduced, their article/paper for the
 author’s personal use, or for company use provided that AAAI copyright 
and the source are indicated, and that the copies are not used in a way 
that implies AAAI endorsement of a product or service of an employer, 
and that the copies per se are not offered for sale. The foregoing right
 shall not permit the posting of the article/paper in electronic or 
digital form on any computer network, except by the author or the 
author’s employer, and then only on the author’s or the employer’s own 
web page or ftp site. Such web page or ftp site, in addition to the 
aforementioned requirements of this Paragraph, must provide an 
electronic reference or link back to the AAAI electronic server, and 
shall not post other AAAI copyrighted materials not of the author’s or 
the employer’s creation (including tables of contents with links to 
other papers) without AAAI’s written permission.  Author(s) may make 
limited distribution of all or portions of their article/paper prior to 
publication.  In the case of work performed under U.S. Government 
contract, AAAI grants the U.S. Government royalty-free permission to 
reproduce all or portions of the above article/paper, and to authorize 
others to do so, for U.S. Government purposes.  In the event the above 
article/paper is not accepted and published by AAAI, or is withdrawn by 
the author(s) before acceptance by AAAI, this agreement becomes null and
 void.</td>
					</tr>
					<tr>
					<th>日期</th>
						<td>2017/02/12</td>
					</tr>
					<tr>
					<th>访问时间</th>
						<td>2019/5/17 上午12:27:07</td>
					</tr>
					<tr>
					<th>馆藏目录</th>
						<td>www.aaai.org</td>
					</tr>
					<tr>
					<th>学术会议名称</th>
						<td>Thirty-First AAAI Conference on Artificial Intelligence</td>
					</tr>
					<tr>
					<th>语言</th>
						<td>en</td>
					</tr>
					<tr>
					<th>摘要</th>
						<td>Although semi-supervised variational autoencoder (SemiVAE) 
works in image classification task, it fails in text classification task
 if using vanilla LSTM as its decoder. From a perspective of 
reinforcement learning, it is verified that the decoder's capability to 
distinguish between different categorical labels is essential. 
Therefore, Semi-supervised Sequential Variational Autoencoder (SSVAE) is
 proposed, which increases the capability by feeding label into its 
decoder RNN at each time-step. Two specific decoder structures are 
investigated and both of them are verified to be effective. Besides, in 
order to reduce the computational complexity in training, a novel 
optimization method is proposed, which estimates the gradient of the 
unlabeled objective function by sampling, along with two variance 
reduction techniques. Experimental results on Large Movie Review Dataset
 (IMDB) and AG's News corpus show that the proposed approach 
significantly improves the classification accuracy compared with 
pure-supervised classifiers, and achieves competitive performance 
against previous advanced methods. State-of-the-art results can be 
obtained by integrating other pretraining-based methods.</td>
					</tr>
					<tr>
					<th>投递标题</th>
						<td>Thirty-First AAAI Conference on Artificial Intelligence</td>
					</tr>
					<tr>
					<th>添加日期</th>
						<td>2019/5/17 上午12:27:07</td>
					</tr>
					<tr>
					<th>修改日期</th>
						<td>2019/7/19 下午9:26:42</td>
					</tr>
				</tbody></table>
				<h3 class="attachments">附件</h3>
				<ul class="attachments">
					<li id="item_WQ5THRXI">Snapshot					</li>
					<li id="item_VFT6H2XV">Xu 等。 - Variational Autoencoder for Semi-Supervised Text C.pdf					</li>
				</ul>
			</li>

		</ul>
	
</body></html>